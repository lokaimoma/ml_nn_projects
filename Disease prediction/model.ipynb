{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease prediction machine learning project\n",
    "## Overview\n",
    "In this project, we aim to develop a classification model to make the work of physicians easier when predicting and or diagnosing diseases in their patients.\n",
    "\n",
    "## Dataset\n",
    "> **TLDR**   \n",
    "> `132` symptoms mapped to `42` diseases\n",
    "> Download dataset [here](https://www.kaggle.com/datasets/marslinoedward/disease-prediction-data/download?datasetVersionNumber=1)\n",
    "\n",
    "The dataset used in this project consists of 133 columns. Of which `132` represents common symptoms to diseases, and a column representing the prognosis of `42` different diseases. The dataset consists of a train and test set.   \n",
    "You can download the dataset [here](https://www.kaggle.com/datasets/marslinoedward/disease-prediction-data/download?datasetVersionNumber=1) on kaggle. After download extract the dataset folder to the same directory as `model.ipynb` file. Directory structure should be as shown below:\n",
    "![Directory structure](directory.png)\n",
    "\n",
    "## Approach\n",
    "- **Load training dataset and preprocess:** Load dataset from disk, preprocess the data and split into feature matrix(X) and labels(y).\n",
    "- **Model training and selection:** Train several classifieers on the training dataset and select the best.\n",
    "- **Best model evaluation:** Evaluate the best model on the test set.\n",
    "- **Save best model to disk:**: Save best model to disk for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itching</th>\n",
       "      <th>skin_rash</th>\n",
       "      <th>nodal_skin_eruptions</th>\n",
       "      <th>continuous_sneezing</th>\n",
       "      <th>shivering</th>\n",
       "      <th>chills</th>\n",
       "      <th>joint_pain</th>\n",
       "      <th>stomach_pain</th>\n",
       "      <th>acidity</th>\n",
       "      <th>ulcers_on_tongue</th>\n",
       "      <th>...</th>\n",
       "      <th>blackheads</th>\n",
       "      <th>scurring</th>\n",
       "      <th>skin_peeling</th>\n",
       "      <th>silver_like_dusting</th>\n",
       "      <th>small_dents_in_nails</th>\n",
       "      <th>inflammatory_nails</th>\n",
       "      <th>blister</th>\n",
       "      <th>red_sore_around_nose</th>\n",
       "      <th>yellow_crust_ooze</th>\n",
       "      <th>Unnamed: 133</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>4920.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.137805</td>\n",
       "      <td>0.159756</td>\n",
       "      <td>0.021951</td>\n",
       "      <td>0.045122</td>\n",
       "      <td>0.021951</td>\n",
       "      <td>0.162195</td>\n",
       "      <td>0.139024</td>\n",
       "      <td>0.045122</td>\n",
       "      <td>0.045122</td>\n",
       "      <td>0.021951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021951</td>\n",
       "      <td>0.021951</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.344730</td>\n",
       "      <td>0.366417</td>\n",
       "      <td>0.146539</td>\n",
       "      <td>0.207593</td>\n",
       "      <td>0.146539</td>\n",
       "      <td>0.368667</td>\n",
       "      <td>0.346007</td>\n",
       "      <td>0.207593</td>\n",
       "      <td>0.207593</td>\n",
       "      <td>0.146539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146539</td>\n",
       "      <td>0.146539</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           itching    skin_rash  nodal_skin_eruptions  continuous_sneezing  \\\n",
       "count  4920.000000  4920.000000           4920.000000          4920.000000   \n",
       "mean      0.137805     0.159756              0.021951             0.045122   \n",
       "std       0.344730     0.366417              0.146539             0.207593   \n",
       "min       0.000000     0.000000              0.000000             0.000000   \n",
       "25%       0.000000     0.000000              0.000000             0.000000   \n",
       "50%       0.000000     0.000000              0.000000             0.000000   \n",
       "75%       0.000000     0.000000              0.000000             0.000000   \n",
       "max       1.000000     1.000000              1.000000             1.000000   \n",
       "\n",
       "         shivering       chills   joint_pain  stomach_pain      acidity  \\\n",
       "count  4920.000000  4920.000000  4920.000000   4920.000000  4920.000000   \n",
       "mean      0.021951     0.162195     0.139024      0.045122     0.045122   \n",
       "std       0.146539     0.368667     0.346007      0.207593     0.207593   \n",
       "min       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000      1.000000     1.000000   \n",
       "\n",
       "       ulcers_on_tongue  ...   blackheads     scurring  skin_peeling  \\\n",
       "count       4920.000000  ...  4920.000000  4920.000000   4920.000000   \n",
       "mean           0.021951  ...     0.021951     0.021951      0.023171   \n",
       "std            0.146539  ...     0.146539     0.146539      0.150461   \n",
       "min            0.000000  ...     0.000000     0.000000      0.000000   \n",
       "25%            0.000000  ...     0.000000     0.000000      0.000000   \n",
       "50%            0.000000  ...     0.000000     0.000000      0.000000   \n",
       "75%            0.000000  ...     0.000000     0.000000      0.000000   \n",
       "max            1.000000  ...     1.000000     1.000000      1.000000   \n",
       "\n",
       "       silver_like_dusting  small_dents_in_nails  inflammatory_nails  \\\n",
       "count          4920.000000           4920.000000         4920.000000   \n",
       "mean              0.023171              0.023171            0.023171   \n",
       "std               0.150461              0.150461            0.150461   \n",
       "min               0.000000              0.000000            0.000000   \n",
       "25%               0.000000              0.000000            0.000000   \n",
       "50%               0.000000              0.000000            0.000000   \n",
       "75%               0.000000              0.000000            0.000000   \n",
       "max               1.000000              1.000000            1.000000   \n",
       "\n",
       "           blister  red_sore_around_nose  yellow_crust_ooze  Unnamed: 133  \n",
       "count  4920.000000           4920.000000        4920.000000           0.0  \n",
       "mean      0.023171              0.023171           0.023171           NaN  \n",
       "std       0.150461              0.150461           0.150461           NaN  \n",
       "min       0.000000              0.000000           0.000000           NaN  \n",
       "25%       0.000000              0.000000           0.000000           NaN  \n",
       "50%       0.000000              0.000000           0.000000           NaN  \n",
       "75%       0.000000              0.000000           0.000000           NaN  \n",
       "max       1.000000              1.000000           1.000000           NaN  \n",
       "\n",
       "[8 rows x 133 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = Path().resolve() / \"Disease Prediction Data\"\n",
    "df = pandas.read_csv(dataset_dir / \"Training.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fungal infection', 'Allergy', 'GERD', 'Chronic cholestasis',\n",
       "       'Drug Reaction', 'Peptic ulcer diseae', 'AIDS', 'Diabetes ',\n",
       "       'Gastroenteritis', 'Bronchial Asthma', 'Hypertension ', 'Migraine',\n",
       "       'Cervical spondylosis', 'Paralysis (brain hemorrhage)', 'Jaundice',\n",
       "       'Malaria', 'Chicken pox', 'Dengue', 'Typhoid', 'hepatitis A',\n",
       "       'Hepatitis B', 'Hepatitis C', 'Hepatitis D', 'Hepatitis E',\n",
       "       'Alcoholic hepatitis', 'Tuberculosis', 'Common Cold', 'Pneumonia',\n",
       "       'Dimorphic hemmorhoids(piles)', 'Heart attack', 'Varicose veins',\n",
       "       'Hypothyroidism', 'Hyperthyroidism', 'Hypoglycemia',\n",
       "       'Osteoarthristis', 'Arthritis',\n",
       "       '(vertigo) Paroymsal  Positional Vertigo', 'Acne',\n",
       "       'Urinary tract infection', 'Psoriasis', 'Impetigo'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(\"Unnamed: 133\", axis=1)\n",
    "df[\"prognosis\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training set into feature matrix and labels then ecocde label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4920, 132), (4920,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df[\"prognosis\"]\n",
    "X = df.iloc[:, :-1]\n",
    "X = X.values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"classifier\", \"passthrough\")\n",
    "])\n",
    "\n",
    "RAND_STATE = 69\n",
    "\n",
    "param_grid = [\n",
    "    {\"classifier\": [AdaBoostClassifier(LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\"))]},\n",
    "    {\"classifier\": [RandomForestClassifier(random_state=RAND_STATE, n_jobs=-1)]},\n",
    "    {\"classifier\": [GradientBoostingClassifier()],\n",
    "    \"classifier__learning_rate\": [0.03, 0.01, 0.1, 1.0]}\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy', verbose=2)\n",
    "grid.fit(X, y)\n",
    "model = grid.best_estimator_\n",
    "print(f\"Best accuracy: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00         1\n",
      "           3       1.00      1.00      1.00         1\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         1\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         1\n",
      "           8       1.00      1.00      1.00         1\n",
      "           9       1.00      1.00      1.00         1\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       1.00      1.00      1.00         1\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       1.00      1.00      1.00         1\n",
      "          15       1.00      0.50      0.67         2\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       1.00      1.00      1.00         1\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         1\n",
      "          21       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          23       1.00      1.00      1.00         1\n",
      "          24       1.00      1.00      1.00         1\n",
      "          25       1.00      1.00      1.00         1\n",
      "          26       1.00      1.00      1.00         1\n",
      "          27       0.50      1.00      0.67         1\n",
      "          28       1.00      1.00      1.00         1\n",
      "          29       1.00      1.00      1.00         1\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         1\n",
      "          33       1.00      1.00      1.00         1\n",
      "          34       1.00      1.00      1.00         1\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       1.00      1.00      1.00         1\n",
      "          37       1.00      1.00      1.00         1\n",
      "          38       1.00      1.00      1.00         1\n",
      "          39       1.00      1.00      1.00         1\n",
      "          40       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.98        42\n",
      "   macro avg       0.99      0.99      0.98        42\n",
      "weighted avg       0.99      0.98      0.98        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df = pandas.read_csv(dataset_dir / \"Testing.csv\")\n",
    "y_test = label_encoder.transform(testing_df[\"prognosis\"])\n",
    "X_test = testing_df.iloc[:, :-1]\n",
    "X_test = X_test.values\n",
    "test_predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store best model and label encoder for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_estimator.pickle.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier=RandomForestClassifier(n_jobs=-1, random_state=69); total time=   0.8s\n",
      "[CV] END classifier=RandomForestClassifier(n_jobs=-1, random_state=69); total time=   0.8s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.03; total time= 2.9min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.1; total time= 2.4min\n",
      "[CV] END classifier=AdaBoostClassifier(estimator=LogisticRegression(multi_class='multinomial')); total time=   9.3s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.03; total time= 2.8min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.1; total time= 2.5min\n",
      "[CV] END classifier=RandomForestClassifier(n_jobs=-1, random_state=69); total time=   0.8s\n",
      "[CV] END classifier=RandomForestClassifier(n_jobs=-1, random_state=69); total time=   0.8s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.03; total time= 2.7min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.01; total time= 2.7min\n",
      "[CV] END classifier=AdaBoostClassifier(estimator=LogisticRegression(multi_class='multinomial')); total time=  10.4s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.01; total time= 2.8min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.1; total time= 2.4min\n",
      "[CV] END classifier=AdaBoostClassifier(estimator=LogisticRegression(multi_class='multinomial')); total time=   9.0s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.03; total time= 2.8min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.01; total time= 2.6min\n",
      "[CV] END classifier=AdaBoostClassifier(estimator=LogisticRegression(multi_class='multinomial')); total time=  10.4s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.01; total time= 3.0min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.1; total time= 2.3min\n",
      "[CV] END classifier=AdaBoostClassifier(estimator=LogisticRegression(multi_class='multinomial')); total time=   9.3s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.01; total time= 3.0min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=1.0; total time=  36.4s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=1.0; total time=  39.1s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=1.0; total time=  39.0s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=1.0; total time=  27.9s\n",
      "[CV] END classifier=RandomForestClassifier(n_jobs=-1, random_state=69); total time=   0.8s\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.03; total time= 2.9min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=0.1; total time= 2.4min\n",
      "[CV] END classifier=GradientBoostingClassifier(), classifier__learning_rate=1.0; total time=  16.0s\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(label_encoder, \"label_encoder.pickle.gz\", compress=3)\n",
    "joblib.dump(model, \"best_estimator.pickle.gz\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
